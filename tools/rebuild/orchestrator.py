"""
Rebuild Orchestrator.

Coordinates all rebuild modules to execute staged rebuild process.
"""

import os
import sys
import json
import time
import logging

# LÃ¤gg till project root i path fÃ¶r imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from tools.rebuild.file_manager import FileManager
from tools.rebuild.process_manager import ServiceManager, CompletionWatcher
from services.review.interactive_review import run_interactive_review, apply_review_decisions
from services.utils.graph_service import GraphStore

LOGGER = logging.getLogger('RebuildOrchestrator')


def _log(msg):
    """Helper fÃ¶r att logga med timestamp."""
    from datetime import datetime
    print(f"{datetime.now().strftime('[%H:%M:%S]')} {msg}")


class RebuildOrchestrator:
    """Orkestrerar rebuild-processen."""
    
    def __init__(self, phase, config):
        self.phase = phase
        self.config = config
        self.file_manager = FileManager(config)
        self.service_manager = ServiceManager(config)
        self.completion_watcher = CompletionWatcher(config, self.file_manager.manifest)
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.staging_info = {}
    
    def _run_graph_builder(self):
        """KÃ¶r graf-byggning direkt i samma process fÃ¶r att ha kontroll Ã¶ver GraphStore-anslutningar."""
        _log("  ğŸ§  KÃ¶r Graf-byggning...")
        
        # Importera och kÃ¶r direkt istÃ¤llet fÃ¶r subprocess
        # Detta lÃ¶ser DuckDB-lÃ¥skonflikter eftersom alla GraphStore-anslutningar sker i samma process
        try:
            from services.indexers.graph_builder import process_lake_batch
            process_lake_batch()
            _log("  âœ… Graf-byggning klar")
        except Exception as e:
            _log(f"  âŒ Graf-byggning fel: {e}")
            LOGGER.error(f"Graph Builder Error: {e}", exc_info=True)
            raise RuntimeError(f"Graf-byggning misslyckades: {e}") from e
    
    def _run_dreamer(self):
        """KÃ¶r dreamer fÃ¶r konsolidering direkt i samma process."""
        _log("  ğŸ’­ KÃ¶r Dreamer...")
        
        # Importera och kÃ¶r consolidate() direkt istÃ¤llet fÃ¶r subprocess
        # Detta ger oss tillgÃ¥ng till review_list i returvÃ¤rdet
        try:
            from services.processors.dreamer import consolidate
            result = consolidate()
            
            # consolidate() returnerar en dict med stats inklusive review_list
            review_list = result.get("review_list", [])
            status = result.get("status", "OK")
            
            if status == "OK":
                _log(f"  âœ… Dreamer klar: {len(review_list)} entiteter behÃ¶ver granskning")
            elif status == "NO_AI":
                _log("  âš ï¸ Dreamer klar men AI-klient saknas")
            else:
                _log(f"  âš ï¸ Dreamer status: {status}")
            
            return {
                "status": status,
                "review_list": review_list,
                "stats": result
            }
        except ImportError as e:
            error_msg = f"Dreamer import fel: {e}"
            _log(f"  âš ï¸ {error_msg}")
            LOGGER.error(f"HARDFAIL: {error_msg}")
            return {"status": "ERROR", "error": str(e), "review_list": []}
        except Exception as e:
            error_msg = f"Dreaming misslyckades: {e}"
            _log(f"  âš ï¸ {error_msg}")
            LOGGER.error(f"HARDFAIL: {error_msg}", exc_info=True)
            return {"status": "ERROR", "error": str(e), "review_list": []}
    
    def run(self, days_limit=None, use_multipass=False):
        """KÃ¶r rebuild-processen."""
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        _log(f"  STAGED REBUILD - Fas: {self.phase.upper()}")
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # 1. Initiera Manifest
        self.file_manager.manifest.set_phase(self.phase)
        
        # 2. Samla filer fÃ¶r fasen
        _log("\nğŸ“ Samlar filer...")
        all_files = self.file_manager.get_all_source_files(self.phase)
        if not all_files:
            _log("âŒ Inga filer att processa fÃ¶r denna fas.")
            return

        # Registrera targets i manifest
        all_uuids = [f['uuid'] for f in all_files]
        self.file_manager.manifest.add_targets(all_uuids)
        
        pending_files = [f for f in all_files if not self.file_manager.manifest.is_complete(f['uuid'])]
        _log(f"   Totalt {len(all_files)} filer, {len(pending_files)} Ã¥terstÃ¥r att processa.")
        
        if not pending_files:
            _log("âœ… Alla filer i denna fas Ã¤r redan klara.")
            return

        # 3. Gruppera PENDING files per datum (vi behÃ¶ver inte processa klara dagar)
        files_by_date = self.file_manager.group_files_by_date(all_files)  # Gruppera ALLA fÃ¶r att kunna Ã¥terstÃ¤lla rÃ¤tt
        sorted_dates = sorted(files_by_date.keys())
        
        if days_limit:
            sorted_dates = sorted_dates[:days_limit]
            _log(f"   BegrÃ¤nsat till {days_limit} dagar.")

        # 4. Flytta ALLA filer till staging (fÃ¶r att tÃ¶mma assets)
        _log("\nğŸ“¦ Flyttar filer till staging...")
        self.staging_info = self.file_manager.move_to_staging(all_files)
        
        if use_multipass:
            os.environ['DOC_CONVERTER_MULTIPASS'] = '1'
            self.config.setdefault("processing", {})["multipass_enabled"] = True
            _log("   ğŸ”¬ Multipass-extraktion aktiverad")

        try:
            for i, date in enumerate(sorted_dates, 1):
                day_files = files_by_date[date]
                
                # Kolla om dagens filer redan Ã¤r klara
                day_pending = [f for f in day_files if not self.file_manager.manifest.is_complete(f['uuid'])]
                if not day_pending:
                    # Dagen Ã¤r helt klar, men vi mÃ¥ste Ã¤ndÃ¥ Ã¥terstÃ¤lla filerna frÃ¥n staging 
                    # sÃ¥ de ligger rÃ¤tt i Assets (annars fÃ¶rsvinner de vid cleanup).
                    # Men vi behÃ¶ver inte starta tjÃ¤nster.
                    _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date} (Redan klar)")
                    self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                    continue

                _log(f"\n{'â”€' * 50}")
                _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date}")
                _log(f"   {len(day_pending)} filer att indexera (av {len(day_files)})")
                
                # Starta tjÃ¤nster FÃ–RST sÃ¥ att watchdogs Ã¤r redo nÃ¤r filer Ã¥terstÃ¤lls
                _log("   ğŸš€ Startar tjÃ¤nster...")
                service_start_time = time.time()
                self.service_manager.start(self.phase)
                service_start_duration = time.time() - service_start_time
                
                # Kort paus fÃ¶r att tjÃ¤nsterna ska starta och watchdogs ska vara redo
                time.sleep(2)
                LOGGER.info(f"DEBUG: TjÃ¤nster startade, Ã¥terstÃ¤ller nu filer...")
                
                # Ã…terstÃ¤ll filer EFTER att tjÃ¤nsterna startat (sÃ¥ watchdogs ser dem som nya)
                _log("   ğŸ“‚ Ã…terstÃ¤ller dagens filer...")
                self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                
                # Verifiera att filerna faktiskt finns i Assets efter Ã¥terstÃ¤llning
                for f in day_pending:
                    if os.path.exists(f['path']):
                        LOGGER.info(f"DEBUG: Fil verifierad i Assets efter Ã¥terstÃ¤llning: {f['path']}")
                    else:
                        LOGGER.error(f"DEBUG: Fil saknas i Assets efter Ã¥terstÃ¤llning: {f['path']}")
                
                LOGGER.info(f"DEBUG: TjÃ¤nster startade, vÃ¤ntar nu pÃ¥ filprocessering...")
                
                # VÃ¤nta pÃ¥ completion
                try:
                    self.completion_watcher.wait_for_completion(day_files, date)
                except RuntimeError as e:
                    _log(f"\nâŒ {e}")
                    self.service_manager.stop()
                    raise
                
                self.service_manager.stop()
                
                # Konsolidering
                self._run_graph_builder()
                dreamer_result = self._run_dreamer()
                
                # Interaktiv granskning om det finns entiteter att granska
                review_list = dreamer_result.get("review_list", [])
                LOGGER.info(f"DEBUG: Dreamer returnerade {len(review_list)} entiteter fÃ¶r granskning")
                if review_list:
                    _log("   ğŸ” Granskning av fÃ¶rslag...")
                    # LÃ¤s taxonomy_path frÃ¥n config (Princip 8)
                    taxonomy_path = os.path.expanduser(self.config['paths']['taxonomy_file'])
                    
                    # Ladda taxonomy
                    if os.path.exists(taxonomy_path):
                        with open(taxonomy_path, 'r', encoding='utf-8') as f:
                            taxonomy = json.load(f)
                    else:
                        taxonomy = {}
                    
                    # Ã–ppna GraphStore fÃ¶r att spara validation rules
                    graph_path = os.path.expanduser(self.config['paths']['graph_db'])
                    graph = GraphStore(graph_path, read_only=False)
                    try:
                        # KÃ¶r interaktiv granskning (skicka med taxonomy fÃ¶r automatisk godkÃ¤nnande)
                        review_decisions = run_interactive_review(review_list, taxonomy)
                        
                        # Applicera beslut
                        if review_decisions:
                            apply_review_decisions(taxonomy, review_decisions, graph)
                            
                            # Spara uppdaterad taxonomy
                            if taxonomy:
                                with open(taxonomy_path, 'w', encoding='utf-8') as f:
                                    json.dump(taxonomy, f, ensure_ascii=False, indent=2)
                            
                            _log(f"   âœ… {len(review_decisions)} beslut applicerade")
                    finally:
                        graph.close()
                
                _log(f"   âœ… Dag {date} klar!")
                
            _log(f"\n{'â•' * 50}")
            _log("ğŸ‰ FAS KLAR!")
            
        finally:
            self.service_manager.stop()
            if self.staging_info:
                _log("\nğŸ“‚ Ã…terstÃ¤ller kvarvarande filer...")
                self.file_manager.restore_all_from_staging(self.staging_info)
            self.file_manager.cleanup_staging()

