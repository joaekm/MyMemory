"""
Rebuild Orchestrator.

Coordinates all rebuild modules to execute staged rebuild process.
Uses ingestion_engine.process_document() for consistent pipeline.

Refactored as part of OBJEKT-73.
"""

import os
import sys
import json
import time
import logging

# LÃ¤gg till project root i path fÃ¶r imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from tools.rebuild.file_manager import FileManager
from tools.rebuild.process_manager import CompletionWatcher
from services.utils.graph_service import GraphService
from services.utils.shared_lock import resource_lock, clear_stale_locks

LOGGER = logging.getLogger('RebuildOrchestrator')


def _log(msg):
    """Helper fÃ¶r att logga med timestamp."""
    from datetime import datetime
    print(f"{datetime.now().strftime('[%H:%M:%S]')} {msg}")


class RebuildOrchestrator:
    """Orkestrerar rebuild-processen."""

    def __init__(self, phase, config):
        self.phase = phase
        self.config = config
        self.file_manager = FileManager(config)
        self.completion_watcher = CompletionWatcher(config, self.file_manager.manifest)
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.staging_info = {}

        # Clear stale locks from previous crashed runs
        clear_stale_locks()
    
    def _run_dreamer(self):
        """KÃ¶r Dreamer (Entity Resolver) fÃ¶r att stÃ¤da grafen."""
        _log("  ğŸ˜´ KÃ¶r Dreamer (StÃ¤dning & LÃ¤nkning)...")
        try:
            from services.utils.vector_service import VectorService
            from services.engines.dreamer import Dreamer

            # Ladda paths frÃ¥n config
            graph_path = os.path.expanduser(self.config['paths']['graph_db'])

            # Ta lÃ¥s fÃ¶r hela Dreamer-cykeln
            with resource_lock("graph", exclusive=True):
                with resource_lock("vector", exclusive=True):
                    # Initiera tjÃ¤nster
                    graph_service = GraphService(graph_path)
                    vector_service = VectorService()
                    dreamer = Dreamer(graph_service, vector_service)

                    # KÃ¶r cykel
                    stats = dreamer.run_resolution_cycle(dry_run=False)

                    _log(f"  âœ… Dreamer klar: Merged={stats.get('merged', 0)}, Renamed={stats.get('renamed', 0)}")
                    graph_service.close()

            # Reset counter after Dreamer run
            try:
                from services.engines.ingestion_engine import reset_dreamer_counter
                reset_dreamer_counter()
            except ImportError:
                pass

        except Exception as e:
            _log(f"  âŒ KRITISKT Dreamer-fel: {e}")
            LOGGER.error(f"Dreamer Error: {e}", exc_info=True)
            raise RuntimeError(f"HARDFAIL: Dreamer failed: {e}") from e
    
    def run(self, days_limit=None, use_multipass=False):
        """KÃ¶r rebuild-processen."""
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        _log(f"  STAGED REBUILD - Fas: {self.phase.upper()}")
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        # Reset Dreamer counter to prevent daemon from triggering during rebuild
        try:
            from services.engines.ingestion_engine import reset_dreamer_counter
            reset_dreamer_counter()
            _log("   ğŸ”„ Dreamer-rÃ¤knare nollstÃ¤lld")
        except ImportError:
            LOGGER.warning("Could not import reset_dreamer_counter")
        
        # 1. Initiera Manifest
        self.file_manager.manifest.set_phase(self.phase)
        
        # 2. Samla filer fÃ¶r fasen
        _log("\nğŸ“ Samlar filer...")
        all_files = self.file_manager.get_all_source_files(self.phase)
        if not all_files:
            _log("âŒ Inga filer att processa fÃ¶r denna fas.")
            return

        # Registrera targets i manifest
        all_uuids = [f['uuid'] for f in all_files]
        self.file_manager.manifest.add_targets(all_uuids)
        
        pending_files = [f for f in all_files if not self.file_manager.manifest.is_complete(f['uuid'])]
        _log(f"   Totalt {len(all_files)} filer, {len(pending_files)} Ã¥terstÃ¥r att processa.")
        
        if not pending_files:
            _log("âœ… Alla filer i denna fas Ã¤r redan klara.")
            return

        # 3. Gruppera PENDING files per datum (vi behÃ¶ver inte processa klara dagar)
        files_by_date = self.file_manager.group_files_by_date(all_files)  # Gruppera ALLA fÃ¶r att kunna Ã¥terstÃ¤lla rÃ¤tt
        sorted_dates = sorted(files_by_date.keys())
        
        # Filtrera bort helt klara dagar INNAN vi applicerar days_limit
        # Annars fastnar vi pÃ¥ samma dag om den redan Ã¤r klar
        pending_dates = []
        completed_dates = []
        for date in sorted_dates:
            day_files = files_by_date[date]
            day_pending = [f for f in day_files if not self.file_manager.manifest.is_complete(f['uuid'])]
            if day_pending:
                pending_dates.append(date)
            else:
                completed_dates.append(date)
        
        if days_limit:
            # BegrÃ¤nsa till X PENDING dagar, men inkludera alla completed fÃ¶r Ã¥terstÃ¤llning
            dates_to_process = pending_dates[:days_limit]
            sorted_dates = completed_dates + dates_to_process  # FÃ¶rst completed (fÃ¶r cleanup), sen pending
            _log(f"   BegrÃ¤nsat till {days_limit} pending dagar ({len(pending_dates)} totalt).")


        # 4. Flytta ALLA filer till staging (fÃ¶r att tÃ¶mma assets)
        _log("\nğŸ“¦ Flyttar filer till staging...")
        self.staging_info = self.file_manager.move_to_staging(all_files)
        
        if use_multipass:
            os.environ['DOC_CONVERTER_MULTIPASS'] = '1'
            self.config.setdefault("processing", {})["multipass_enabled"] = True
            _log("   ğŸ”¬ Multipass-extraktion aktiverad")

        try:
            for i, date in enumerate(sorted_dates, 1):
                day_files = files_by_date[date]
                
                # Kolla om dagens filer redan Ã¤r klara
                day_pending = [f for f in day_files if not self.file_manager.manifest.is_complete(f['uuid'])]
                if not day_pending:
                    # Dagen Ã¤r helt klar, men vi mÃ¥ste Ã¤ndÃ¥ Ã¥terstÃ¤lla filerna frÃ¥n staging 
                    # sÃ¥ de ligger rÃ¤tt i Assets (annars fÃ¶rsvinner de vid cleanup).
                    # Men vi behÃ¶ver inte starta tjÃ¤nster.
                    _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date} (Redan klar)")
                    self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                    continue

                _log(f"\n{'â”€' * 50}")
                _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date}")
                _log(f"   {len(day_pending)} filer att indexera (av {len(day_files)})")

                # Ã…terstÃ¤ll dagens filer frÃ¥n staging till Assets
                _log("   ğŸ“‚ Ã…terstÃ¤ller dagens filer...")
                self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                
                # DIREKT PROCESSING: AnvÃ¤nd ingestion_engine.process_document()
                # Detta sÃ¤kerstÃ¤ller samma pipeline som realtids-ingestion (OBJEKT-73)
                _log(f"   ğŸ”§ Processar {len(day_pending)} filer via IngestionEngine...")
                try:
                    from services.engines.ingestion_engine import process_document

                    # Ta lÃ¥s pÃ¥ graph och vector fÃ¶r hela dagens batch
                    with resource_lock("graph", exclusive=True):
                        with resource_lock("vector", exclusive=True):
                            for f in day_pending:
                                if os.path.exists(f['path']):
                                    _log(f"      â†’ {f['filename']}")
                                    try:
                                        # _lock_held=True eftersom vi redan har lÃ¥sen
                                        process_document(f['path'], f['filename'], _lock_held=True)
                                    except Exception as doc_err:
                                        LOGGER.error(f"HARDFAIL: Fel vid processning av {f['filename']}: {doc_err}")
                                        raise RuntimeError(f"HARDFAIL: Document processing failed for {f['filename']}: {doc_err}") from doc_err
                except ImportError as e:
                    LOGGER.error(f"HARDFAIL: Kunde inte importera IngestionEngine: {e}")
                    raise RuntimeError(f"IngestionEngine import failed: {e}")

                # VÃ¤nta pÃ¥ att filer dyker upp i Lake (verifiering)
                try:
                    self.completion_watcher.wait_for_completion(day_files, date)
                except RuntimeError as e:
                    _log(f"\nâŒ {e}")
                    raise

                # StÃ¤dning (Dreamer) - kÃ¶rs med eget lÃ¥s
                self._run_dreamer()
                
                _log(f"   âœ… Dag {date} klar!")
                
            _log(f"\n{'â•' * 50}")
            _log("ğŸ‰ FAS KLAR!")

        finally:
            if self.staging_info:
                _log("\nğŸ“‚ Ã…terstÃ¤ller kvarvarande filer...")
                self.file_manager.restore_all_from_staging(self.staging_info)
            self.file_manager.cleanup_staging()

