"""
Rebuild Orchestrator.

Coordinates all rebuild modules to execute staged rebuild process.
"""

import os
import sys
import json
import time
import logging

# LÃ¤gg till project root i path fÃ¶r imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from tools.rebuild.file_manager import FileManager
from tools.rebuild.process_manager import ServiceManager, CompletionWatcher
from services.utils.graph_service import GraphService

LOGGER = logging.getLogger('RebuildOrchestrator')


def _log(msg):
    """Helper fÃ¶r att logga med timestamp."""
    from datetime import datetime
    print(f"{datetime.now().strftime('[%H:%M:%S]')} {msg}")


class RebuildOrchestrator:
    """Orkestrerar rebuild-processen."""
    
    def __init__(self, phase, config):
        self.phase = phase
        self.config = config
        self.file_manager = FileManager(config)
        self.service_manager = ServiceManager(config)
        self.completion_watcher = CompletionWatcher(config, self.file_manager.manifest)
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.staging_info = {}
    
    def _run_dreamer(self):
        """KÃ¶r Dreamer (Entity Resolver) fÃ¶r att stÃ¤da grafen."""
        _log("  ğŸ˜´ KÃ¶r Dreamer (StÃ¤dning & LÃ¤nkning)...")
        try:
            from services.utils.graph_service import GraphService
            from services.utils.vector_service import VectorService
            from services.engines.dreamer import Dreamer

            # Ladda paths frÃ¥n config
            graph_path = os.path.expanduser(self.config['paths']['graph_db'])

            # Initiera tjÃ¤nster
            graph_service = GraphService(graph_path)
            vector_service = VectorService()
            dreamer = Dreamer(graph_service, vector_service)

            # KÃ¶r cykel
            stats = dreamer.run_resolution_cycle(dry_run=False)
            
            _log(f"  âœ… Dreamer klar: Merged={stats['merged']}, Reviewed={stats['reviewed']}")
            graph_service.close()
            
        except Exception as e:
            _log(f"  âš ï¸  Dreamer fel (Icke-kritiskt): {e}")
            LOGGER.error(f"Dreamer Error: {e}", exc_info=True)
            # Vi lÃ¥ter inte Dreamer-fel stoppa hela rebuilden, men vi loggar det.
    
    def run(self, days_limit=None, use_multipass=False):
        """KÃ¶r rebuild-processen."""
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        _log(f"  STAGED REBUILD - Fas: {self.phase.upper()}")
        _log("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # 1. Initiera Manifest
        self.file_manager.manifest.set_phase(self.phase)
        
        # 2. Samla filer fÃ¶r fasen
        _log("\nğŸ“ Samlar filer...")
        all_files = self.file_manager.get_all_source_files(self.phase)
        if not all_files:
            _log("âŒ Inga filer att processa fÃ¶r denna fas.")
            return

        # Registrera targets i manifest
        all_uuids = [f['uuid'] for f in all_files]
        self.file_manager.manifest.add_targets(all_uuids)
        
        pending_files = [f for f in all_files if not self.file_manager.manifest.is_complete(f['uuid'])]
        _log(f"   Totalt {len(all_files)} filer, {len(pending_files)} Ã¥terstÃ¥r att processa.")
        
        if not pending_files:
            _log("âœ… Alla filer i denna fas Ã¤r redan klara.")
            return

        # 3. Gruppera PENDING files per datum (vi behÃ¶ver inte processa klara dagar)
        files_by_date = self.file_manager.group_files_by_date(all_files)  # Gruppera ALLA fÃ¶r att kunna Ã¥terstÃ¤lla rÃ¤tt
        sorted_dates = sorted(files_by_date.keys())
        
        # Filtrera bort helt klara dagar INNAN vi applicerar days_limit
        # Annars fastnar vi pÃ¥ samma dag om den redan Ã¤r klar
        pending_dates = []
        completed_dates = []
        for date in sorted_dates:
            day_files = files_by_date[date]
            day_pending = [f for f in day_files if not self.file_manager.manifest.is_complete(f['uuid'])]
            if day_pending:
                pending_dates.append(date)
            else:
                completed_dates.append(date)
        
        if days_limit:
            # BegrÃ¤nsa till X PENDING dagar, men inkludera alla completed fÃ¶r Ã¥terstÃ¤llning
            dates_to_process = pending_dates[:days_limit]
            sorted_dates = completed_dates + dates_to_process  # FÃ¶rst completed (fÃ¶r cleanup), sen pending
            _log(f"   BegrÃ¤nsat till {days_limit} pending dagar ({len(pending_dates)} totalt).")


        # 4. Flytta ALLA filer till staging (fÃ¶r att tÃ¶mma assets)
        _log("\nğŸ“¦ Flyttar filer till staging...")
        self.staging_info = self.file_manager.move_to_staging(all_files)
        
        if use_multipass:
            os.environ['DOC_CONVERTER_MULTIPASS'] = '1'
            self.config.setdefault("processing", {})["multipass_enabled"] = True
            _log("   ğŸ”¬ Multipass-extraktion aktiverad")

        try:
            for i, date in enumerate(sorted_dates, 1):
                day_files = files_by_date[date]
                
                # Kolla om dagens filer redan Ã¤r klara
                day_pending = [f for f in day_files if not self.file_manager.manifest.is_complete(f['uuid'])]
                if not day_pending:
                    # Dagen Ã¤r helt klar, men vi mÃ¥ste Ã¤ndÃ¥ Ã¥terstÃ¤lla filerna frÃ¥n staging 
                    # sÃ¥ de ligger rÃ¤tt i Assets (annars fÃ¶rsvinner de vid cleanup).
                    # Men vi behÃ¶ver inte starta tjÃ¤nster.
                    _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date} (Redan klar)")
                    self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                    continue

                _log(f"\n{'â”€' * 50}")
                _log(f"ğŸ“… DAG {i}/{len(sorted_dates)}: {date}")
                _log(f"   {len(day_pending)} filer att indexera (av {len(day_files)})")
                
                # Starta tjÃ¤nster FÃ–RST sÃ¥ att watchdogs Ã¤r redo nÃ¤r filer Ã¥terstÃ¤lls
                _log("   ğŸš€ Startar tjÃ¤nster...")
                service_start_time = time.time()
                self.service_manager.start(self.phase)
                service_start_duration = time.time() - service_start_time
                
                # Kort paus fÃ¶r att tjÃ¤nsterna ska starta och watchdogs ska vara redo
                time.sleep(2)
                LOGGER.info(f"DEBUG: TjÃ¤nster startade, Ã¥terstÃ¤ller nu filer...")
                
                # Ã…terstÃ¤ll filer EFTER att tjÃ¤nsterna startat (sÃ¥ watchdogs ser dem som nya)
                _log("   ğŸ“‚ Ã…terstÃ¤ller dagens filer...")
                self.file_manager.restore_files_for_date(date, files_by_date, self.staging_info)
                
                # Verifiera att filerna faktiskt finns i Assets efter Ã¥terstÃ¤llning
                for f in day_pending:
                    if os.path.exists(f['path']):
                        LOGGER.info(f"DEBUG: Fil verifierad i Assets efter Ã¥terstÃ¤llning: {f['path']}")
                    else:
                        LOGGER.error(f"DEBUG: Fil saknas i Assets efter Ã¥terstÃ¤llning: {f['path']}")
                
                # DIREKT PROCESSING: Bypaassa watchdog helt!
                # macOS FSEvents Ã¤r opÃ¥litligt, sÃ¥ vi anropar DocConverter direkt
                _log(f"   ğŸ”§ Triggar DocConverter direkt fÃ¶r {len(day_pending)} filer...")
                try:
                    # VIKTIGT: Importera modulen FÃ–RST och initiera GATEKEEPER INNAN vi importerar funktioner
                    # Annars fÃ¥r funktionerna en None-referens till GATEKEEPER
                    import services.processors.doc_converter as dc_module
                    
                    # Initiera Gatekeeper om den inte redan Ã¤r initierad
                    if dc_module.GATEKEEPER is None:
                        _log("      ğŸ“¦ Initierar Gatekeeper...")
                        dc_module.GATEKEEPER = dc_module.EntityGatekeeper()
                        _log(f"      âœ“ Gatekeeper redo")
                    
                    for f in day_pending:
                        if os.path.exists(f['path']):
                            _log(f"      â†’ {f['filename']}")
                            dc_module.processa_dokument(f['path'], f['filename'])
                except ImportError as e:
                    LOGGER.warning(f"Kunde inte importera DocConverter direkt: {e}")
                    # Fallback: vÃ¤nta pÃ¥ watchdog
                    _log("   â³ Fallback: VÃ¤ntar pÃ¥ att watchdogs ska upptÃ¤cka filer...")
                    time.sleep(5)

                
                LOGGER.info(f"DEBUG: Filer processade, vÃ¤ntar nu pÃ¥ att de ska dyka upp i Lake...")

                
                # VÃ¤nta pÃ¥ completion
                try:
                    self.completion_watcher.wait_for_completion(day_files, date)
                except RuntimeError as e:
                    _log(f"\nâŒ {e}")
                    self.service_manager.stop()
                    raise
                
                self.service_manager.stop()

                # StÃ¤dning (Dreamer)
                self._run_dreamer()
                
                _log(f"   âœ… Dag {date} klar!")
                
            _log(f"\n{'â•' * 50}")
            _log("ğŸ‰ FAS KLAR!")
            
        finally:
            self.service_manager.stop()
            if self.staging_info:
                _log("\nğŸ“‚ Ã…terstÃ¤ller kvarvarande filer...")
                self.file_manager.restore_all_from_staging(self.staging_info)
            self.file_manager.cleanup_staging()

